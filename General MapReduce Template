import java.io.*;
import java.util.*;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reporter;

/*
* Map Class of MapReduce framework code
*/
public class Maps extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable>
{
	private final static IntWritable one = new IntWritable(1);

    private Text word = new Text();
    
	public void map(LongWritable key, Text value,OutputCollector<Text, IntWritable> output, Reporter reporter)throws IOException {
		
		    String line = value.toString();
		    //StringTokenizer tokenizer = new StringTokenizer(line,","); //Separate words between ','
		    StringTokenizer tokenizer = new StringTokenizer(line);
        
        while (tokenizer.hasMoreTokens({
        	word.set(tokenizer.nextToken());
        	context.write(word, one);
        }
	}
}

============================= A New File ====================================================================== 

import java.io.*;
import java.util.*;


import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.mapred.Reducer;

/*
* Reduce Class of MapReduce framework code
*/
public class Reduces extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable>
{

	public void reduce(Text key, Iterator<IntWritable> values,OutputCollector<Text, IntWritable> output, Reporter reporter)throws IOException {   	
	  int sum = 0;
		while (values.hasNext()){
			sum += values.next().get();
		}
		output.collect(key, new IntWritable(sum));
	}

}


======================================== A New File ===============================================================


import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.TextInputFormat;
import org.apache.hadoop.mapred.TextOutputFormat;

/*
* Main Class of the MapReduce program to schedule the Map and Reduce jobs
*/
public class Main {
	public static void main(String[] args) throws Exception{
		 JobConf conf = new JobConf(Snworks.class);
		 conf.setJobName("socialnetwork");
		 long start = System.currentTimeMillis();
		 conf.setOutputKeyClass(Text.class);
		 conf.setOutputValueClass(IntWritable.class);
		 
		 conf.setMapperClass(Maps.class);
		 conf.setCombinerClass(Reduces.class);
		 conf.setReducerClass(Reduces.class);
		 
		 conf.setInputFormat(TextInputFormat.class);
		 conf.setOutputFormat(TextOutputFormat.class);
		 
		 FileInputFormat.setInputPaths(conf, new Path(args[0]));
		 FileOutputFormat.setOutputPath(conf, new Path(args[1]));
		 
		 JobClient.runJob(conf);
	   long end = System.currentTimeMillis();
	   long sec = (end - start)/1000;
	   long mins = sec/60;
	   System.out.println(mins + " : " + sec);
	 }
}
